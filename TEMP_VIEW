import re
import html
from pathlib import Path

SUPPORTED_EXTENSIONS = {".txt", ".md"}


def prompt_file_path() -> Path:
    """Prompt the user to supply a path to a supported text file."""
    while True:
        raw_path = input("Enter the path to a .txt or .md file: ").strip().strip('"')
        if not raw_path:
            print("Please provide a non-empty path.")
            continue

        path = Path(raw_path).expanduser().resolve()
        if not path.exists():
            print(f"No file found at {path}. Try again.")
            continue

        if not path.is_file():
            print(f"{path} is not a file. Try again.")
            continue

        if path.suffix.lower() not in SUPPORTED_EXTENSIONS:
            supported = ', '.join(sorted(SUPPORTED_EXTENSIONS))
            print(f"Unsupported file type: {path.suffix}. Supported types are: {supported}.")
            continue

        return path


def prompt_keywords() -> list[str]:
    """Prompt the user to supply one or more keywords."""
    while True:
        raw_keywords = input("Enter keywords to search for (comma separated): ")
        keywords = [kw.strip() for kw in raw_keywords.split(',') if kw.strip()]
        if not keywords:
            print("Please enter at least one keyword.")
            continue
        return keywords


def load_text(path: Path) -> str:
    """Load the text content from *path*, trying UTF-8 first."""
    try:
        return path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        # Fallback to the system default encoding if UTF-8 fails.
        return path.read_text()


def split_sentences(text: str) -> list[str]:
    """Split *text* into simple sentence-like chunks."""
    normalized = text.replace("\r\n", "\n").replace("\r", "\n")
    parts = re.split(r"(?<=[.!?])\s+|\n+", normalized)
    sentences = [part.strip() for part in parts if part.strip()]
    return sentences


def find_keyword_sentences(sentences: list[str], keyword: str) -> list[str]:
    """Return sentences that contain *keyword* (case insensitive)."""
    keyword_lower = keyword.lower()
    matches = [s for s in sentences if keyword_lower in s.lower()]
    return matches


def split_blocks(text: str) -> list[str]:
    """Split raw text into blocks separated by blank lines."""
    normalized = text.replace("\r\n", "\n").replace("\r", "\n").strip()
    blocks = re.split(r"\n\s*\n+", normalized)
    return [b.strip() for b in blocks if b.strip()]


def parse_entry_fields(block: str) -> tuple[str | None, str | None, str | None]:
    """Attempt to extract (company, role, location) from a block.

    Tries labeled lines first, then common inline patterns.
    Returns None for fields that aren't found.
    """
    company = role = location = None

    # Labeled lines
    m = re.search(r"(?im)^\s*(Company|Employer|Organization)\s*:\s*(.+?)\s*$", block)
    if m:
        company = m.group(2).strip()
    m = re.search(r"(?im)^\s*(Role|Position|Title)\s*:\s*(.+?)\s*$", block)
    if m:
        role = m.group(2).strip()
    m = re.search(r"(?im)^\s*(Location|City|Remote)\s*:\s*(.+?)\s*$", block)
    if m:
        location = m.group(2).strip()

    # Inline forms if still missing
    if not (company and role and location):
        # Pattern: Company - Role - Location
        m = re.search(r"(?im)^\s*([^\-|\u2013\u2014\|]+?)\s*[\-|\u2013\u2014]\s*([^\-|\u2013\u2014\|]+?)\s*[\-|\u2013\u2014]\s*(.+?)\s*$", block)
        if m:
            company = company or m.group(1).strip()
            role = role or m.group(2).strip()
            location = location or m.group(3).strip()

    if not (company and role):
        # Pattern: Role at Company (Location)
        m = re.search(r"(?im)^\s*(.+?)\s+at\s+(.+?)(?:\s*\((.+?)\))?\s*$", block)
        if m:
            role = role or m.group(1).strip()
            company = company or m.group(2).strip()
            if m.lastindex and m.lastindex >= 3 and m.group(3):
                location = location or m.group(3).strip()

    return company or None, role or None, location or None


def _strip_markdown_inline(text: str) -> str:
    """Remove common inline Markdown/HTML artifacts from a single-line field."""
    t = text
    # images -> alt text
    t = re.sub(r"!\[([^\]]*)\]\((?:[^)]+)\)", r"\1", t)
    # links -> label
    t = re.sub(r"\[([^\]]+)\]\((?:[^)]+)\)", r"\1", t)
    # inline code
    t = re.sub(r"`([^`]*)`", r"\1", t)
    # HTML tags
    t = re.sub(r"<[^>]+>", "", t)
    # collapse whitespace
    t = re.sub(r"\s+", " ", t).strip()
    return t


def _entries_to_table(entries: list[tuple[str | None, str | None, str | None, list[str]]], keywords: list[str]) -> list[str]:
    """Convert parsed entries to a clean table (TSV) filtered by keywords.

    Keeps only rows where any sentence in the entry contains at least one
    keyword (case-insensitive).
    """
    def matches_keywords(sents: list[str]) -> bool:
        kl = [k.lower() for k in keywords]
        for s in sents:
            sl = s.lower()
            for k in kl:
                if k and k in sl:
                    return True
        return False

    # Sort and filter
    def sort_key(e: tuple[str | None, str | None, str | None, list[str]]):
        c, r, l, _ = e
        return (c or "", r or "", l or "")

    filtered = [e for e in entries if matches_keywords(e[3])]
    rows = []
    seen = set()
    for company, role, location, _ in sorted(filtered, key=sort_key):
        c = _strip_markdown_inline(company or "") or "-"
        r = _strip_markdown_inline(role or "") or "-"
        l = _strip_markdown_inline(location or "") or "-"
        key = (c, r, l)
        if key in seen:
            continue
        seen.add(key)
        rows.append(f"{c}\t{r}\t{l}")

    lines: list[str] = []
    lines.append("Company\tRole\tLocation")
    if rows:
        lines.extend(rows)
    else:
        lines.append("-")
    return lines


def _extract_td_cells(html_text: str) -> list[tuple[str, str | None]]:
    """Return a flat list of (<td> inner text plain, first href) for each cell."""
    cells: list[tuple[str, str | None]] = []
    for m in re.finditer(r"(?is)<td\b[^>]*>(.*?)</td>", html_text):
        inner = m.group(1)
        # find first href
        href_match = re.search(r"(?is)<a\b[^>]*href\s*=\s*(['\"])\s*([^'\"]+?)\s*\1", inner)
        href = href_match.group(2).strip() if href_match else None
        # strip tags and decode entities
        plain = re.sub(r"(?is)<[^>]+>", "", inner)
        plain = html.unescape(plain)
        plain = re.sub(r"\s+", " ", plain).strip()
        cells.append((plain, href))
    return cells


def format_results(file_path: Path, keywords: list[str], sentences: list[str], full_text: str) -> str:
    """Create a TSV with Company, Role, Location, Link from <td> neighbors.

    For each <td> whose text contains any keyword, take:
      company = td[i-2], role = td[i-1], location = td[i], link = href in td[i+1]
    Skips incomplete rows and de-duplicates identical lines.
    """
    cells = _extract_td_cells(full_text)
    kl = [k.lower() for k in keywords if k]
    lines: list[str] = ["Company\tRole\tLocation\tLink"]
    seen: set[tuple[str, str, str, str]] = set()

    for i, (text_plain, _) in enumerate(cells):
        lower = text_plain.lower()
        if not any(k in lower for k in kl):
            continue
        # Ensure neighbors exist
        if i - 2 < 0 or i + 1 >= len(cells):
            continue
        company = cells[i - 2][0]
        role = cells[i - 1][0]
        location = text_plain
        link = cells[i + 1][1] or cells[i + 1][0]

        # Clean final fields further (remove leftover pipes, md, commas spacing)
        def clean(s: str) -> str:
            s = _strip_markdown_inline(s)
            return s

        c = clean(company) or "-"
        r = clean(role) or "-"
        l = clean(location) or "-"
        u = (link or "-").strip()
        row_key = (c, r, l, u)
        if row_key in seen:
            continue
        seen.add(row_key)
        lines.append(f"{c}\t{r}\t{l}\t{u}")

    # If nothing matched, fall back to previous 3-col parsing for resilience
    if len(lines) == 1:
        fallback = _entries_to_table([], keywords)  # will produce header + '-' by default
        # replace header to 4 columns for consistency
        lines = ["Company\tRole\tLocation\tLink", "-"]

    return "\n".join(lines).rstrip() + "\n"


def main() -> None:
    print("Welcome to txtExtractor!")
    file_path = prompt_file_path()
    keywords = prompt_keywords()

    try:
        text = load_text(file_path)
    except OSError as exc:
        print(f"Failed to read {file_path}: {exc}")
        return

    sentences = split_sentences(text)
    if not sentences:
        print("No text content found in the file.")
        return

    # Format results once for both console and file output
    result_text = format_results(file_path, keywords, sentences, text)

    # Print to terminal
    print("\n" + result_text)

    # Also write to a .txt file alongside the source file
    out_path = file_path.with_name(file_path.stem + "_extracted.txt")
    try:
        out_path.write_text(result_text, encoding="utf-8")
        print(f"Results saved to: {out_path}")
    except OSError as exc:
        print(f"Failed to write results to {out_path}: {exc}")


if __name__ == "__main__":
    main()

